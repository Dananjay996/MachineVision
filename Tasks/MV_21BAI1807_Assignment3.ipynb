{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"line-height: 1.2;\">\n",
    "\n",
    "### **BCSE417P Lab Assignment 3**\n",
    "\n",
    "#### **Name**: SreeDananjay S\n",
    "\n",
    "#### **Reg No**: 21BAI1807\n",
    "\n",
    "##### **Nov 9, 2024**\n",
    "\n",
    "##### By turning in this assignment, I agree and declare that all of this is my own work.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '1.1build1'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.1.3 which is incompatible.\n",
      "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Video Loading\n",
    "The first step is to load the provided video file using OpenCV’s `cv2.VideoCapture()` function, which reads and captures the video for processing. We also retrieve key video properties, such as frame count and frame rate, to understand the video’s structure and adjust processing based on its frame rate if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 23.976023976023978)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load video\n",
    "video_path = './5738706-hd_1920_1080_24fps.mp4'\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
    "frame_count, frame_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Frame Extraction**\n",
    "\n",
    "#### Step 2: Frame Extraction\n",
    "To analyze each frame individually, we extract frames from the video in a loop until all frames are read. Each frame is stored in a list for later processing. This is achieved by reading each frame in a `while` loop until there are no more frames to read (`success` becomes `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "success, frame = video.read()\n",
    "while success:\n",
    "    frames.append(frame)\n",
    "    success, frame = video.read()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Spatio-Temporal Segmentation**\n",
    "\n",
    "#### Step 3: Spatio-Temporal Segmentation\n",
    "This step segments objects within each frame to observe changes over time. We use edge detection here (using the Canny edge detection method) to find object boundaries, but color thresholding or other techniques can also be applied. \n",
    "Each processed frame is added to `segmented_frames` for visualization and tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_frames = []\n",
    "for frame in frames:\n",
    "    # Convert to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply edge detection\n",
    "    edges = cv2.Canny(gray_frame, threshold1=50, threshold2=150)\n",
    "    segmented_frames.append(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Scene Cut Detection**\n",
    "\n",
    "#### Step 4: Scene Cut Detection\n",
    "We use a histogram-based approach to detect abrupt and gradual scene transitions. By calculating and comparing histograms between consecutive frames, we identify frames where the difference is significant enough to mark a scene cut. A threshold is set to detect large changes (hard cuts) and gradually changing intensities (soft cuts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_cuts = []\n",
    "hist_prev = cv2.calcHist([frames[0]], [0], None, [256], [0, 256])\n",
    "\n",
    "for i in range(1, len(frames)):\n",
    "    hist_curr = cv2.calcHist([frames[i]], [0], None, [256], [0, 256])\n",
    "    hist_diff = cv2.compareHist(hist_prev, hist_curr, cv2.HISTCMP_CORREL)\n",
    "\n",
    "    if hist_diff < 0.5:  # Threshold for hard cut\n",
    "        scene_cuts.append(i)\n",
    "\n",
    "    # Update previous histogram\n",
    "    hist_prev = hist_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Mark Scene Cuts**\n",
    "\n",
    "#### Step 5: Mark Scene Cuts\n",
    "Once the scene cuts are identified, we annotate frames where the cuts were detected to make them visually recognizable. We display a message like \"Scene Cut\" on these frames and store them in `scene_summary` for visual presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_summary = []\n",
    "for cut_frame in scene_cuts:\n",
    "    summary_frame = frames[cut_frame].copy()\n",
    "    cv2.putText(summary_frame, 'Scene Cut', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    scene_summary.append(summary_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **Result Visualization**\n",
    "\n",
    "#### Step 6: Result Visualization\n",
    "This step visualizes our results by displaying frames with detected scene cuts and selected segmented frames. We use OpenCV's `cv2.imshow()` function to show the frames sequentially. Each scene cut frame is displayed for 500 ms, and we similarly display a few segmented frames for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in scene_summary:\n",
    "    cv2.imshow(\"Scene Cut\", frame)\n",
    "    cv2.waitKey(500)  # Display each scene cut frame for 500 ms\n",
    "\n",
    "for frame in segmented_frames[:10]:  # Display first 5 segmented frames\n",
    "    cv2.imshow(\"Segmentation\", frame)\n",
    "    cv2.waitKey(500)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
